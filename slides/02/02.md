title: NPFL114, Lecture 2
class: title, langtech, cc-by-nc-sa
# Training Neural Networks

## Milan Straka

### March 11, 2019

---
section: ML Basics
# Machine Learning Basics

We usually have a **training set**, which is assumed to consist of examples
generated independently from a **data generating distribution**.

The goal of _optimization_ is to match the training set as well as possible.

~~~
However, the main goal of _machine learning_ is to perform well on _previously
unseen_ data, so called **generalization error** or **test error**. We typically
estimate the generalization error using a **test set** of examples independent
of the training set.

---
# Machine Learning Basics

Challenges in machine learning:
- _underfitting_
- _overfitting_

~~~
![w=80%,h=center](underfitting_overfitting.pdf)

---
# Machine Learning Basics

We control whether a model underfits or overfits by modifying its _capacity_.
- representational capacity
- effective capacity

~~~
![w=60%,h=center](generalization_error.pdf)

~~~
The **No free lunch theorem** (Wolpert, 1996) states that averaging over
_all possible_ data distributions, every classification algorithm achieves
the same _overall_ error when processing unseen examples. In a sense, no machine
learning algorithm is _universally_ better than any other.

---
# Machine Learning Basics

Any change in a machine learning algorithm that is designed to _reduce
generalization error_ but not necessarily its training error is called
**regularization**.

~~~

$L_2$ regularization (also called weighted decay) penalizes models
with large weights (i.e., penalty of $||→θ||^2$).

![w=70%,h=center](regularization.pdf)

---
# Machine Learning Basics

_Hyperparameters_ are not adapted by learning algorithm itself.

Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing to update hyperparameters accordingly.

---
section: Loss
# Loss Function

A model is usually trained in order to minimize the _loss_ on the training data.

~~~

Assuming that a model computes $f(→x;→θ)$ using parameters $→θ$,
the _mean square error_ is computed as
$$∑_i \left(f(→x^{(i)}; →θ) - y^{(i)}\right)^2.$$

~~~

A common principle used to design loss functions is the _maximum likelihood
principle_.

---
# Maximum Likelihood Estimation

Let $𝕏 = \\{→x^{(1)}, →x^{(2)}, \ldots, →x^{(m)}\\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$. We denote
the empirical data distribution as $\hat p_\textrm{data}$.

Let $p_\textrm{model}(→x; →θ)$ be a family of distributions. The
*maximum likelihood estimation* of $→θ$ is:

$$\begin{aligned}
→θ_\mathrm{ML} &= \argmax_→θ p_\textrm{model}(\mathbb X; →θ) \\
               &= \argmax_→θ ∏\nolimits_{i=1}^m p_\textrm{model}(→x^{(i)}; →θ) \\
               &= \argmin_→θ ∑\nolimits_{i=1}^m -\log p_\textrm{model}(→x^{(i)}; →θ) \\
               &= \argmin_→θ 𝔼_{⁇→x ∼ p̂_\textrm{data}} [-\log p_\textrm{model}(→x; →θ)] \\
               &= \argmin_→θ H(p̂_\textrm{data}, p_\textrm{model}(→x; →θ)) \\
               &= \argmin_→θ D_\textrm{KL}(p̂_\textrm{data}||p_\textrm{model}(→x; →θ)) \color{gray} + H(p̂_\textrm{data})
\end{aligned}$$

---
# Maximum Likelihood Estimation

Easily generalized to situations where our goal is predict $y$ given $→x$.
$$\begin{aligned}
→θ_\mathrm{ML} &= \argmax_→θ p_\textrm{model}(\mathbb Y | \mathbb X; →θ) \\
               &= \argmax_→θ ∏\nolimits_{i=1}^m p_\textrm{model}(y^{(i)} | →x^{(i)}; →θ) \\
               &= \argmin_→θ ∑\nolimits_{i=1}^m -\log p_\textrm{model}(y^{(i)} | →x^{(i)}; →θ) \\
\end{aligned}$$

The resulting _loss function_ is called _negative log likelihood_, or
_cross-entropy_ or _Kullback-Leibler divegence_.

---
# Mean Square Error as MLE

Assume our goal is to perform a regression, i.e., to predict $p(y | →x)$ for $y ∈ ℝ$.

Let $ŷ(→x; →θ)$ give a prediction of mean of $y$.

We define $p(y | →x)$ as $\N(y; ŷ(→x; →θ), σ^2)$ for a given fixed $σ$.
Then:
$$\begin{aligned}
\argmax_→θ p(y | →x; →θ) =& \argmin_→θ ∑_{i=1}^m -\log p(y^{(i)} | →x^{(i)} ; →θ) \\
                          =& -\argmin_→θ ∑_{i=1}^m \log \sqrt{\frac{1}{2πσ^2}}
                            e ^ {\normalsize -\frac{(y^{(i)} - ŷ(→x^{(i)}; →θ))^2}{2σ^2}} \\
                          =& -m \log σ^{-1} -m \log (2π)^{-1/2} - \argmin_→θ ∑_{i=1}^m -\frac{(y^{(i)} - ŷ(→x^{(i)}; →θ))^2}{2σ^2} \\
                          =& \argmin_→θ ∑_{i=1}^m \frac{||y^{(i)} - ŷ(→x^{(i)}; →θ)||^2}{2σ^2}
\end{aligned}$$
